{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afda9d34-7492-403f-bff6-5fd0376ef2cc",
   "metadata": {},
   "source": [
    "# Accelerated Machine Learning\n",
    "\n",
    "Welcome to the Accelerated Machine Learning course at AIMS 2025.\n",
    "\n",
    "This tutorial is based on the hls4ml tutorial notebooks (https://github.com/fastmachinelearning/hls4ml-tutorial/). The dataset is described in more detail in this article (https://iopscience.iop.org/article/10.1088/1748-0221/13/07/P07027/pdf). By the end of the course you will have:\n",
    "\n",
    "* Trained a neural network to identify different types of \"jets\" at the Large Hadron Collider\n",
    "* Used the hls4ml python library to convert the neural network to FPGA firmware\n",
    "* Examined the FPGA resource usage for the neural network\n",
    "* Understood floating point vs fixed point precision and the implications for inference accuracy and resource usage\n",
    "* Implemented quantization-aware training using the QKeras library\n",
    "* Tested the effect of \"pruning\" on the inference accuray and resource usage\n",
    "\n",
    "## Course assessment\n",
    "As well as the exercises (highlighted in <span style=\"color: green;\">green</span>), there will be a number of questions throughout the notebook to answer. The assessed questions are highlighted in <span style=\"color: red;\">red</span>, whilst the unassessed questions are highlighted in <span style=\"color: orange;\">orange</span>. **Both are important for your understanding**. Please upload the answers to the assessed questions via this Google Forms link: https://forms.gle/RYC28gBfhTdsqFT67.\n",
    "\n",
    "The course assessment is out of 30. The marks for each question are provided in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87730f69-a704-44b8-a2be-61f6b4b24dae",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the jet-tagging dataset\n",
    "\n",
    "In this section we will import the dependencies and load the jet-tagging dataset. Following this we will take some time to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126a00e-d49d-4cd5-b8c5-6218477332dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "import ndjson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d660038-adaf-41e2-bda1-646792c9836b",
   "metadata": {},
   "source": [
    "Jets are one of the most important objects that we reconstruct at the Large Hadron Collider experiments. \n",
    "When fundamental particles, known as quarks or gluons, are produced in the proton-proton collisions,\n",
    "they undergo a process called \"hadronisation\".\n",
    "This creates narrow sprays of particles (hadrons) which are referred to as \"jets\".\n",
    "By reconstructing and understanding these jets, we can infer properties of the fundamental particle interactions.\n",
    "The image below shows a reconstruction of a collision event which produces two back-to-back jets with extremely high energy in the CMS detector.\n",
    "\n",
    "<img src=\"figures/event_display_dijet.png\" alt=\"Dijet event display\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13b282-cce5-48da-8d83-9176e48b4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = fetch_openml('hls4ml_lhc_jets_hlf', as_frame=False, parser='liac-arff')\n",
    "X, y = data['data'], data['target']\n",
    "labels = np.unique(y)\n",
    "features = data['feature_names']\n",
    "\n",
    "print(\"Number of jets:\", len(X))\n",
    "print(\"Number of features:\", len(features))\n",
    "print(\"Different target classes:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5795e5-20dc-40e9-a273-1043e7209bc8",
   "metadata": {},
   "source": [
    "These correspond to jets produced from a gluon (q), light-quark (q), top-quark decay (t->bqq), W-boson decay (w->qq') and a Z-boson decay (z->qq). \n",
    "Jets from the different classes will have different substructure, and we can use this information for identification.\n",
    "A summary of the properties included in the dataset is provided in the table below.\n",
    "\n",
    "<img src=\"figures/table_observables.png\" alt=\"Table of observables\" width=\"100\"/>\n",
    "\n",
    "The properties are described in this reference (https://arxiv.org/abs/1709.08705), but it is not important for you to know the exact details for this tutorial. Just that we have five different classes to predict, each of which have a different distribution in the high-dimensional jet property phase space.\n",
    "\n",
    "Let's have a look at the 1D distributions for the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34376ca2-a442-4951-905e-645b90696069",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"#E69F00\",  # orange\n",
    "    \"#56B4E9\",  # sky blue\n",
    "    \"#009E73\",  # green\n",
    "    \"#CC79A7\",  # pink/purple\n",
    "    \"#D55E00\",  # red/orange\n",
    "]\n",
    "\n",
    "def plot_input_feature(ax, X, y, features, x=\"zlogz\", classes=['g','q','t','w','z'], bins=40):\n",
    "    fi = np.where(np.array(features)==x)[0][0]\n",
    "    vals = X.T[fi]\n",
    "\n",
    "    # Obtain percentiles for sensible plot\n",
    "    xlo, xhi = np.percentile(vals,0.5), np.percentile(vals,99.5)\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        mask = (y == c)\n",
    "        ax.hist(vals[mask], bins=bins, range=(xlo,xhi), label=c, color=colors[i], histtype='step')\n",
    "\n",
    "    ax.set_ylabel(\"Entries\")\n",
    "    ax.set_xlabel(x)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(20,20))\n",
    "for k, feature in enumerate(features):\n",
    "    i = int(np.floor(k/4))\n",
    "    j = k%4\n",
    "    plot_input_feature(axs[i][j], X, y, features, x=feature)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6da688-c6fd-4da9-a85e-61df121a8cc9",
   "metadata": {},
   "source": [
    "## Part 2: Create a simple neural network for jet-tagging (offline)\n",
    "\n",
    "We are going to use the 16-dimensional information in the dataset to classify the different kinds of jets.\n",
    "Let's do this with a simple feed-forward multi-layer perceptron.\n",
    "\n",
    "First we need to do a bit of pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c556d1e-7c1b-435c-9267-cd70a6f353f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical labels and do the test-train split\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.02, random_state=42)\n",
    "\n",
    "# Do standard (z-score) scaling of the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the datasets into the results folder for quick loading\n",
    "np.save('results/X_train_val.npy', X_train_val)\n",
    "np.save('results/X_test.npy', X_test)\n",
    "np.save('results/y_train_val.npy', y_train_val)\n",
    "np.save('results/y_test.npy', y_test)\n",
    "np.save('results/classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1f087-2cf8-4297-a643-6e68881b2dcb",
   "metadata": {},
   "source": [
    "We will now load the relevant functionalities from tensorflow, and then build and train a neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77687719-c67a-41bb-be9d-78c56f3da861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c2333-f7ce-42b8-971d-d7425fa76677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu1'))\n",
    "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu2'))\n",
    "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu3'))\n",
    "model.add(Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "# Print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8da184-d4be-4172-8b9e-58955719d6dc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "    \n",
    "### Questions (unassessed)\n",
    "* Do you understand the architecture of this MLP?\n",
    "* What do the `kernel_initializer` and `kernal_regularizer` arguments for the dense layers do?\n",
    "* Why might an L1 regularisation be appropriate for a network with limited resources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e05f23-585c-4a07-8ce3-a69615eeed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "N_epochs = 20\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a1151-d303-4c9c-964e-6944e0db113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    batch_size=128,\n",
    "    epochs=N_epochs,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557894ae-7cc2-4f8c-9d9d-bca3c8304b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "history = result.history\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "epochs = np.linspace(1, N_epochs, N_epochs)\n",
    "axs[0].plot(epochs, history['loss'], c='blue', label='Train')\n",
    "axs[0].plot(epochs, history['val_loss'], c='orange', label='Val')\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "axs[1].plot(epochs, history['accuracy'], c='blue', label='Train')\n",
    "axs[1].plot(epochs, history['val_accuracy'], c='orange', label='Val')\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy [%]\")\n",
    "axs[1].legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6207e-d696-4a51-b75a-3dd5d8b2d933",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "\n",
    "### Questions (unassessed)\n",
    "* What conclusions can you draw from the shape of the loss curve?\n",
    "    * **Hint**: comment on overtraining/undertraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02fadea-c745-46a4-ab2b-1121b6095685",
   "metadata": {},
   "source": [
    "This tutorial is not about obtaining the best performance for the \"software\" neural network classifier, so we will not spend any more time improving the performance of this network. In any case, we should evaluate the performance on the test dataset to provide a \"software\" baseline.\n",
    "\n",
    "Let's first calculate the global accuracy. This is defined as the fraction of correctly classified jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68558a87-befd-4cad-90a1-37ee6dd1bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_keras = model.predict(X_test)\n",
    "keras_accuracy = np.sum(np.argmax(y_test, axis=1) == np.argmax(y_keras, axis=1))/y_test.shape[0]\n",
    "\n",
    "print(f\"Achieved test accuracy of {(100 * keras_accuracy):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99870b27-4633-4d54-90b1-78721566bd63",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "Let's understand the performance in more detail beyond the global accuracy. It is interesting to evaluate how the model classifies one type of jet vs another. In this exercise, we will:\n",
    "\n",
    "1) Plot the output probabilities from the neural network (multi-)classifier\n",
    "2) Evaluate the ROC curves\n",
    "3) Plot the \"confusion matrix\" which tells us the fraction of predicted labels for each \"truth\" jet type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a334466-aacc-4cf9-94bc-2d09b153bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Build masks for truth class\n",
    "masks = {}\n",
    "for truth_label in labels:\n",
    "    masks[truth_label] = np.sum((labels == truth_label)*y_test, axis=1, dtype='bool')\n",
    "\n",
    "fig, axs = plt.subplots(3,2, figsize=(12,12))\n",
    "axs[2,1].axis('off')\n",
    "\n",
    "# Loop over predicted classes and plot the probabilities from the classifier\n",
    "# Store the probabilities for future use\n",
    "probs = {}\n",
    "for j, pred_label in enumerate(list(labels)):\n",
    "\n",
    "    x = int(np.floor(j/2))\n",
    "    y = j%2\n",
    "    \n",
    "    arg = np.where(labels==pred_label)[0][0]\n",
    "    prob = y_keras.T[arg]\n",
    "    probs[pred_label] = prob\n",
    "\n",
    "    lo, hi = 0, 1\n",
    "\n",
    "    # Plot probability distribution for each truth class\n",
    "    for i, truth_label in enumerate(list(labels)):\n",
    "        mask = masks[truth_label]\n",
    "        axs[x][y].hist(prob[mask], bins=40, range=(lo,hi), label=truth_label, histtype='step', color=colors[i])\n",
    "\n",
    "    axs[x][y].set_xlabel(f\"Prob({pred_label})\")\n",
    "    axs[x][y].set_ylabel(\"Entries\")\n",
    "    axs[x][y].set_yscale(\"log\")\n",
    "    axs[x][y].legend(loc='lower left', title=\"Truth class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd4810-762c-4f79-aedc-179ea59c5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fig, axs = plt.subplots(3,2, figsize=(12,16))\n",
    "axs[2,1].axis('off')\n",
    "\n",
    "for j, label in enumerate(list(labels)):\n",
    "\n",
    "    x = int(np.floor(j/2))\n",
    "    y = j%2\n",
    "\n",
    "    y_true = masks[label]\n",
    "    y_pred = probs[label]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    axs[x][y].plot(fpr, tpr, label=f\"All others (AUC={auc:.2f})\", color='black')\n",
    "    for i in range(0, len(thresholds), 200):\n",
    "        axs[x][y].text(fpr[i], tpr[i], f\"{thresholds[i]:.2f}\", fontsize=6, ha='right', va='bottom', color='black')\n",
    "\n",
    "    # Plot for each pair\n",
    "    for other_label in labels:\n",
    "        if other_label == label:\n",
    "            continue\n",
    "\n",
    "        mask_pair = masks[label]+masks[other_label]\n",
    "        fpr, tpr, thresholds = roc_curve(y_true[mask_pair], y_pred[mask_pair])\n",
    "        auc = roc_auc_score(y_true[mask_pair], y_pred[mask_pair])\n",
    "        axs[x][y].plot(fpr, tpr, label=f\"{other_label} (AUC={auc:.2f})\", color=colors[np.where(labels==other_label)[0][0]], ls='--')\n",
    "\n",
    "    axs[x][y].set_xlabel(\"False positive rate\")\n",
    "    axs[x][y].set_ylabel(\"True positive rate\")\n",
    "    axs[x][y].set_title(f\"{label} vs X (using Prob({label}))\")\n",
    "    axs[x][y].legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fe498-3efd-4b37-a42b-ba78980536e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "# This tell us the fraction of each true class landing in each predicted class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true_label = np.argmax(y_test, axis=1)\n",
    "y_pred_label = np.argmax(y_keras, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true_label, y_pred_label, normalize='true')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel('Predicted class')\n",
    "ax.set_ylabel('True class')\n",
    "ax.set_title(\"Normalized by true class\")\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a29b7-888f-44a6-9cb2-d4a2c6109396",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCC\">\n",
    "\n",
    "### Questions (assessed)\n",
    "\n",
    "These quiz questions will test your understanding of the results we have just obtained.\n",
    "\n",
    "* Which true jet class is most often misidentified as a gluon-jet (g)? [1 mark]\n",
    "* What do the small numbers along the ROC curves represent? [1 mark]\n",
    "* Which true jet class is easiest to identify? What is special about the substructure of this type of jet compared to the others? [2 marks]\n",
    "\n",
    "Please add your answers to the Google Forms linked at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb89aa-7a7c-4717-8537-2cfdf97a860f",
   "metadata": {},
   "source": [
    "## Part 3: Online jet-tagging -- convert the model to FPGA firmware\n",
    "\n",
    "As discussed in the lectures, we will benefit hugely from implementing Machine Learning in the CMS trigger - the part of the detector that decides which data should be written to disk. For this \"online\" jet tagging, we need to synthesize the neural network classifier trained in the previous section into FPGA firmware, and ensure that it fits within the latency and resource usage requirements.\n",
    "\n",
    "In this section we will convert the neural network trained with tensorflow into FPGA firmware. This is known as High-Level Synthesis (HLS) which allows us to write software-like code (e.g. in C, C++) and automatically compile it into hardware code (logic gates, registers, interconnects) that can be synthesized on an FPGA.\n",
    "\n",
    "We will use the `hls4ml` package with an Intel OneAPI backend to do this. We have heard about these packages in the lecture slides. The tutorial will focus on optimising the resource usage, as latency tests require a full simulation of the data flow within the FPGA.\n",
    "\n",
    "Before starting, there's one key property of FPGA algorithms which must be highlighted.\n",
    "\n",
    "### Datatypes in HLS\n",
    "Fixed point representation is used instead of floating point. For example the `hls4ml` default `fixed<16,6>` corresponds to:\n",
    "```\n",
    "+-------------------------+---------------------------+\n",
    "|  Integer part (6 bits)  | Fractional part (10 bits) |\n",
    "+-------------------------+---------------------------+\n",
    "| 101010                  |                1010101010 |\n",
    "+-------------------------+---------------------------+\n",
    "|           Full bitwidth (16 bits)                   |\n",
    "+-------------------------+---------------------------+\n",
    "```\n",
    "You can read more here: https://github.com/hlslibs/ac_types/blob/v3.7/pdfdocs/ac_datatypes_ref.pdf\n",
    "\n",
    "This has important consequences for the synthesized models. At the end of this section we will see how the performance and resource-usage depend on bit-width length used for the network weights and biases during inference. In part 4, we will see how to incorporate bit-widths into the training process to maintain performance for a very small numbers of bits.\n",
    "\n",
    "Let's set up the converter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb6557-1736-4761-83ab-d58eb81fac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model', backend='oneAPI')\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9752268-de32-473d-9e94-3c66dd85eb4d",
   "metadata": {},
   "source": [
    "What do the different items in the configuration dictionary correspond to? The most important for you to understand are:\n",
    "\n",
    "* `Precision`: specifies the numeric format to use in the generated hardware. This choice affects both resource usage and model accuracy. The precision can be specified globally (as shown above) or separate for each neural network layer.\n",
    "* `ReuseFactor`: controls hardware reuse, such that `1` means no reuse i.e. the design uses as many hardware resources as needed to execute all operations in parallel. The lower this factor the faster the inference (lower latency) but more FPGA area (i.e. resources) is used. This factor can be tuned to meet the design constraints (parallelisation).\n",
    "* `Strategy`: tells hls4ml to optimize for the lowest possible latency. Other strategies include `Resource`, which optimizes for minimal area.\n",
    "\n",
    "We will now convert the keras model we trained for an Intel \"Agilex7\" FPGA, a family of high-end FPGAs which are designed for performance-demanding and power-efficient applications, like AI acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf697159-0dfc-4588-82e6-1d4f2808f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, backend='oneAPI', output_dir='model_1/hls4ml_prj', part='Agilex7'\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18665ed1-be18-421c-a6ce-9342e3ffeddb",
   "metadata": {},
   "source": [
    "Now that we have converted the model, we can evaluate the global accuracy (i.e. fraction of correctly classified jets). This should show little change from the software model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843f2b3-e11a-43fa-8687-55740982510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.ascontiguousarray(X_test)\n",
    "y_hls = hls_model.predict(X_test)\n",
    "\n",
    "hls_accuracy = np.sum(np.argmax(y_test, axis=1) == np.argmax(y_hls, axis=1))/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1680b-73b5-430b-b463-ee2819ec583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Achieved test accuracy of {(100 * hls_accuracy):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01533ec6-2f11-4f33-90a5-2cdc2989fa01",
   "metadata": {},
   "source": [
    "### Create a report of the FPGA resource usage\n",
    "\n",
    "Now that we have a classifier which has been converted (and compiled) for use on an FPGA, we can emulate the design build and estimate the resource usage to see if it is compatible with our resource requirements. The report is produced in the project directory. We will will print out the key information from the build report.\n",
    "* **Note**: ignore the seg fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7312fb-44ef-43df-842d-fa2ec6bcffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(build_type='report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604c5d6-9381-4f99-a78c-279ea93271a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_1/hls4ml_prj/build/myproject.report.prj/reports/resources/json/summary.ndjson\", \"r\") as f:\n",
    "    summary = ndjson.load(f)\n",
    "\n",
    "resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summary))[0]['columns'][1:-1]\n",
    "available = list(filter(lambda x: x[\"name\"] == \"Available\", summary))[0]['data'][:-1]\n",
    "estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summary))[0]['data'][:-1]\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~ Resource usage ~~~~~~~~~~~~~~\")\n",
    "for i, resource in enumerate(resource_names):\n",
    "    print(f\"--> {resource}:\")\n",
    "    print(f\"      * Available resource: {available[i]}\")\n",
    "    print(f\"      * Used resource (estimated): {estimated_resources[i]}\")\n",
    "    print(f\"      * Percentage of used resource (estimated): {100*float(estimated_resources[i])/float(available[i]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de480546-6764-45ff-b009-53c1d369cf7e",
   "metadata": {},
   "source": [
    "The different resource types correspond to:\n",
    "* ALUTs: adaptive look-up tables. Implement logic expressions/functions\n",
    "* FFs: flip-flops. Store one bit of data; used for variables/registers\n",
    "* RAMs: On-chip block RAMs. Used for storing network weights, activations etc.\n",
    "* DSPs: Digital Signal Processing blocks. Optimized for multiply/add functionalities which are ideal for NN layers.\n",
    "* MLABs: Memory Logic Array blocks. Small, distributed memory with lower latency and smaller capacity than RAMs.\n",
    "\n",
    "A simple (reductive) guide for when compiling a neural network with hls4ml. If it uses:\n",
    "* Many DSPs --> You are doing lots of multiple/add operations e.g. dense layers/convolutions\n",
    "* Lots of RAMs/MLABs --> You are storing many activations, weights, or intermediate values.\n",
    "* High ALUT/FF usage --> Your logic is complex e.g. control flow, state machines.\n",
    "\n",
    "**So even a quite simple neural network design (only ~3000 trainable parameters) can take up a sizeable chunk of the FPGA.**\n",
    "* How can we accelerate more complex ML algorithms without being limited by the resources?\n",
    "* How can we store many ML algorithms on the same FPGA?\n",
    "\n",
    "We need to think smarter in terms of the training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713aeb7-7bc6-4628-9adb-901dbf235a41",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "\n",
    "Now you have learnt how to convert a software neural network into FPGA firmware, let's see how changing the bitwidth used for the network layer weights and biases during inference affects the accuracy and resource usage.\n",
    "\n",
    "Your task is to loop over several bitwidths and plot the accuracy and resource usage as a function of bitwidth. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77570323-94e3-4aae-9437-485dc6dcb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: you can base your solution using the skeleton code below\n",
    "# We will fix the number of integer bits at 6, use the rest for fractional component\n",
    "\n",
    "# Loop over a set of bit widths\n",
    "bit_widths = [8, 10, 12, 14, 16]\n",
    "\n",
    "hls_accuracy_list = []\n",
    "hls_summary_list = []\n",
    "\n",
    "for bit_width in bit_widths:\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\" --> Testing bit width: {bit_width}\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    # Set default bit_width in the config\n",
    "    config_tmp = config\n",
    "    config_tmp['Model']['Precision']['default'] = f\"fixed<{bit_width},6>\"\n",
    "\n",
    "    # Convert and compile the model\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    # -- ADD CODE HERE --\n",
    "    hls_accuracy_list.append(accuracy)\n",
    "\n",
    "    # Emulate the build and evaluate the resource usage\n",
    "    # -- ADD CODE HERE\n",
    "    with open(\"PATH-TO-REPORT/summary.ndjson\", \"r\") as f:\n",
    "        summary = ndjson.load(f)\n",
    "    hls_summary_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1ef22-99d1-4ec3-8de6-b01b6898d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Extract resource usage from summary jsons\n",
    "resource_usage = {}\n",
    "for summ in hls_summary_list:\n",
    "    resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summ))[0]['columns'][1:-1]\n",
    "    available = list(filter(lambda x: x[\"name\"] == \"Available\", summ))[0]['data'][:-1]\n",
    "    estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summ))[0]['data'][:-1]\n",
    "\n",
    "    for i, resource in enumerate(resource_names):\n",
    "        r = resource.split(\" \")[0]\n",
    "        if r not in resource_usage:\n",
    "            resource_usage[r] = [float(estimated_resources[i])/float(available[i])]\n",
    "        else:\n",
    "            resource_usage[r].append(float(estimated_resources[i])/float(available[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b9529-3e13-49ce-b0c5-0f175fa04250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Plot accuracy and resource usage as a function of the bit widths\n",
    "fig, axs = plt.subplots(2,3, figsize=(16,8))\n",
    "\n",
    "# Top left: accuracy\n",
    "axs[0][0].plot(bit_widths, 100*np.array(hls_accuracy_list), marker='o')\n",
    "axs[0][0].set_xlabel(\"Bit widths\")\n",
    "axs[0][0].set_ylabel(\"Accuracy [%]\")\n",
    "axs[0][0].axhline(20, color='red', ls='--', label=\"Random choice\")\n",
    "axs[0][0].set_ylim(0,100)\n",
    "axs[0][0].legend(loc='best')\n",
    "\n",
    "# Top middle: ALUT usage\n",
    "axs[0][1].plot(bit_widths, 100*np.array(resource_usage['ALUTs']), marker='o')\n",
    "axs[0][1].set_xlabel(\"Bit widths\")\n",
    "axs[0][1].set_ylabel(\"ALUTs [%]\")\n",
    "\n",
    "# Top right: FF usage\n",
    "axs[0][2].plot(bit_widths, 100*np.array(resource_usage['FFs']), marker='o')\n",
    "axs[0][2].set_xlabel(\"Bit widths\")\n",
    "axs[0][2].set_ylabel(\"FFs [%]\")\n",
    "\n",
    "# Bottom left: BRAM usage\n",
    "axs[1][0].plot(bit_widths, 100*np.array(resource_usage['RAMs']), marker='o')\n",
    "axs[1][0].set_xlabel(\"Bit widths\")\n",
    "axs[1][0].set_ylabel(\"RAMs [%]\")\n",
    "\n",
    "# Top middle: DSP usage\n",
    "axs[1][1].plot(bit_widths, 100*np.array(resource_usage['DSPs']), marker='o')\n",
    "axs[1][1].set_xlabel(\"Bit widths\")\n",
    "axs[1][1].set_ylabel(\"DSPs [%]\")\n",
    "\n",
    "# Top right: MLAB usage\n",
    "axs[1][2].plot(bit_widths, 100*np.array(resource_usage['MLABs']), marker='o')\n",
    "axs[1][2].set_xlabel(\"Bit widths\")\n",
    "axs[1][2].set_ylabel(\"MLABs [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f2bd7-4b8d-4c3e-833b-d421f0b88ec9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCC\">\n",
    "\n",
    "### Questions (assessed)\n",
    "\n",
    "* Please upload a screenshot of the graphs produced in this exercise which show the accuracy and resource-usage (LUTs, FFs, DSPs, BRAM, MLAB) as a function of the bit-widths used during inference. [3 marks]\n",
    "* What conclusions can be drawn from the graphs? [2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e2c1a-49e4-408e-b3ad-ed56d27767c9",
   "metadata": {},
   "source": [
    "## Part 4: Quantization-Aware Training\n",
    "\n",
    "In the previous exercise we have seen the trade-off between accuracy and resource usage when using different bit-widths **during inference**.\n",
    "\n",
    "As discussed in the lecture slides, we can achieve similar levels of accuracy with significantly lower bit widths by **quantizing during the network training**. We will use the QKeras library to emulate fixed-point quantization during the forward pass of the training process. This ensures that network learns to optimally classify the different jet types within the fixed bit-width budget.\n",
    "\n",
    "By lowering the bit widths we can:\n",
    "* Reduce the resource usage (lower memory footprint)\n",
    "* Speed up the inference process (reduce latency)\n",
    "\n",
    "These are essential for AI acceleration using FPGAs! Let's start by importing the `qkeras` library. We will then look at how the `quantized_bit` behaves vs the full floating point equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07e096-143c-49e7-ad1b-c96f7d478c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qkeras\n",
    "from qkeras import QDense\n",
    "from qkeras.quantizers import quantized_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44665eb8-3e32-4ba7-813e-d055938cf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(-4,4,1000), quantized_bits(4,1,1,alpha=1)(np.linspace(-4,4,1000)))\n",
    "ax.plot(np.linspace(-4,4,1000), np.linspace(-4,4,1000))\n",
    "\n",
    "ax.legend([\"Quantized\", \"Floating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd511e82-86b6-44e5-b213-e97fc446c1b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "\n",
    "### Questions (unassessed)\n",
    "* Do you understand what the plot is showing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbda795-2b0a-4938-aa46-d953ae714644",
   "metadata": {},
   "source": [
    "### Creating a quantized model\n",
    "\n",
    "To create a \"quantized\" neural network we using the `QDense` layer. This requires setting the `kernel_quantizer` and `bias_quantizer`, which control how the layer's weights and biases are quantized, respectively i.e. how they are converted from full-precision floating-point numbers to lower-bit fixed-point formats during training and inference.\n",
    "\n",
    "We will reduce the model size substantially by only using 4 bits (1 integer) for each weight and bias in the network. This means each weight and bias can only take 1 of 16 different values (as shown in the plot above). Note, the number of training parameters does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc143510-e4a5-4af8-b7ea-39dac9eeb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = Sequential()\n",
    "qmodel.add(QDense(32, input_shape=(16,),\n",
    "                  name='fc1',\n",
    "                  kernel_quantizer = quantized_bits(4,1,1,alpha=1),\n",
    "                  bias_quantizer = quantized_bits(4,1,1,alpha=1),\n",
    "                  kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "qmodel.add(Activation(activation='relu', name='relu1'))\n",
    "qmodel.add(QDense(32, \n",
    "                  name='fc2', \n",
    "                  kernel_quantizer = quantized_bits(4,1,1,alpha=1),\n",
    "                  bias_quantizer = quantized_bits(4,1,1,alpha=1),                  \n",
    "                  kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "qmodel.add(Activation(activation='relu', name='relu2'))\n",
    "qmodel.add(QDense(32, \n",
    "                  name='fc3', \n",
    "                  kernel_quantizer = quantized_bits(4,1,1,alpha=1),\n",
    "                  bias_quantizer = quantized_bits(4,1,1,alpha=1),                  \n",
    "                  kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "qmodel.add(Activation(activation='relu', name='relu3'))\n",
    "qmodel.add(QDense(5, \n",
    "                  name='output', \n",
    "                  kernel_quantizer = quantized_bits(4,1,1,alpha=1),\n",
    "                  bias_quantizer = quantized_bits(4,1,1,alpha=1),                  \n",
    "                  kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "qmodel.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5ef23-af24-4a18-b952-368725ac9eee",
   "metadata": {},
   "source": [
    "Now let's train the network. With qkeras the fixed-point format is only used for the forward pass, and we revert back to the floating-point format for the backwards pass (backpropagation). This ensures that the loss is smooth and differentiable, with respect to the network weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9ec3c-ccd7-46fa-8ebc-6da6241f7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.0001)\n",
    "qmodel.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "# We will initialise with the weights from the full floating-points model (translated into the quantized_bit format)\n",
    "qmodel.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9de277-e5fe-4081-9a74-db169dfec5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 20\n",
    "result_qkeras = qmodel.fit(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    batch_size=128,\n",
    "    epochs=N_epochs,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cdfb5-cb46-4f19-868c-0805f6abb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history and compare to the normal keras training\n",
    "history_qkeras = result_qkeras.history\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "epochs = np.linspace(1, N_epochs, N_epochs)\n",
    "axs[0].plot(epochs, history['loss'], c='blue', label='Train')\n",
    "axs[0].plot(epochs, history['val_loss'], c='orange', label='Val')\n",
    "axs[0].plot(epochs, history_qkeras['loss'], c='blue', label='Train (qkeras)', ls='--')\n",
    "axs[0].plot(epochs, history_qkeras['val_loss'], c='orange', label='Val (qkeras)', ls='--')\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "axs[1].plot(epochs, history['accuracy'], c='blue', label='Train')\n",
    "axs[1].plot(epochs, history['val_accuracy'], c='orange', label='Val')\n",
    "axs[1].plot(epochs, history_qkeras['accuracy'], c='blue', label='Train (qkeras)', ls='--')\n",
    "axs[1].plot(epochs, history_qkeras['val_accuracy'], c='orange', label='Val (qkeras)', ls='--')\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy [%]\")\n",
    "axs[1].legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5a7b6-ed4a-4734-8ad1-8b5276d166bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "\n",
    "### Questions (unassessed)\n",
    "* What conclusions would you draw by comparing the loss curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc52c4-b1c5-485d-9efd-e3ff12d323cf",
   "metadata": {},
   "source": [
    "Now we can use the hls4ml package to convert the \"quantized\" network into FPGA firmware via HLS. The key difference is that we are now using a more complex configuration, with a separate config per layer name. For this we need to set the `granularity` argument to `\"name\"`. Let's set up the configuration and look at the contents (ignoring the warning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be343af-7593-474e-b0b8-3092bcf79b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(qmodel, granularity='name', backend='oneAPI')\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf729dc-188a-4bce-86b6-8cdd8080d5e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "\n",
    "### Questions (unassessed)\n",
    "* Do you understand what has changed in the configuration for this \"quantization-aware\" network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44f69d-c4fe-4461-a9f9-c7e27bcb99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the HLS conversion\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    qmodel, hls_config=config, backend='oneAPI', output_dir='model_2/hls4ml_prj', part='Agilex7'\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bd5de-df16-456c-a176-02ae17944016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And evaluate the accuracy on the data set\n",
    "X_test = np.ascontiguousarray(X_test)\n",
    "y_hls = hls_model.predict(X_test)\n",
    "\n",
    "hls_accuracy = np.sum(np.argmax(y_test, axis=1) == np.argmax(y_hls, axis=1))/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2266dc0-9132-45d5-8100-3fa2f4cba4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Achieved test accuracy of {(100 * hls_accuracy):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d7ff1-2535-43a5-8575-2aa6444d5576",
   "metadata": {},
   "source": [
    "So we have managed to achieve roughly the same accuracy as the baseline model with much lower bit-widths.\n",
    "\n",
    "Let's build the QKeras trained model and get an estimate for the resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcca54-e397-4d9c-8b3a-57b353abfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(build_type='report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd3a2d-7056-40d4-bf1b-c451efe8ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_2/hls4ml_prj/build/myproject.report.prj/reports/resources/json/summary.ndjson\", \"r\") as f:\n",
    "    summary = ndjson.load(f)\n",
    "\n",
    "resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summary))[0]['columns'][1:-1]\n",
    "available = list(filter(lambda x: x[\"name\"] == \"Available\", summary))[0]['data'][:-1]\n",
    "estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summary))[0]['data'][:-1]\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~ Resource usage ~~~~~~~~~~~~~~\")\n",
    "for i, resource in enumerate(resource_names):\n",
    "    print(f\"--> {resource}:\")\n",
    "    print(f\"      * Available resource: {available[i]}\")\n",
    "    print(f\"      * Used resource (estimated): {estimated_resources[i]}\")\n",
    "    print(f\"      * Percentage of used resource (estimated): {float(estimated_resources[i])/float(available[i]):.2f}%\")\n",
    "\n",
    "resource_usage_qkeras = {}\n",
    "for i,resource in enumerate(resource_names):\n",
    "    r = resource.split(\" \")[0]\n",
    "    resource_usage_qkeras[r] = float(estimated_resources[i])/float(available[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f92636-a9e1-4325-bbed-24a203b695eb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise: \n",
    "\n",
    "How do these resource usage numbers compare to the baseline model?\n",
    "\n",
    "1) Plot the accuracy and resource usage statistics vs bit-widths for the baseline model (previous exercise), adding an extra point for the QKeras trained network\n",
    "2) Plot the ALUTs/FFs usage vs accuracy for different bin-widths (previous exercise), and add the point from the QKeras training\n",
    "\n",
    "What conclusions can be drawn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813b440-101c-4905-bcd2-9aa71f8fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "fig, axs = plt.subplots(2,3, figsize=(16,8))\n",
    "\n",
    "bit_width_qkeras = 4\n",
    "\n",
    "# Top left: accuracy\n",
    "axs[0][0].plot(bit_widths, 100*np.array(hls_accuracy_list), marker='o', label='Baseline')\n",
    "axs[0][0].plot(bit_width_qkeras, 100*np.array(hls_accuracy), marker='*', label='QKeras')\n",
    "axs[0][0].set_xlabel(\"Bit widths\")\n",
    "axs[0][0].set_ylabel(\"Accuracy [%]\")\n",
    "axs[0][0].axhline(20, color='red', ls='--', label=\"Random choice\")\n",
    "axs[0][0].set_ylim(0,100)\n",
    "axs[0][0].legend(loc='best')\n",
    "\n",
    "# Top middle: ALUT usage\n",
    "axs[0][1].plot(bit_widths, 100*np.array(resource_usage['ALUTs']), marker='o', label='Baseline')\n",
    "axs[0][1].plot(bit_width_qkeras, 100*resource_usage_qkeras['ALUTs'], marker='*', label='QKeras')\n",
    "axs[0][1].set_xlabel(\"Bit widths\")\n",
    "axs[0][1].set_ylabel(\"ALUTs [%]\")\n",
    "axs[0][1].legend(loc='best')\n",
    "\n",
    "# Top right: FF usage\n",
    "axs[0][2].plot(bit_widths, 100*np.array(resource_usage['FFs']), marker='o', label='Baseline')\n",
    "axs[0][2].plot(bit_width_qkeras, 100*resource_usage_qkeras['FFs'], marker='*', label='QKeras')\n",
    "axs[0][2].set_xlabel(\"Bit widths\")\n",
    "axs[0][2].set_ylabel(\"FFs [%]\")\n",
    "axs[0][2].legend(loc='best')\n",
    "\n",
    "# Bottom left: RAM usage\n",
    "axs[1][0].plot(bit_widths, 100*np.array(resource_usage['RAMs']), marker='o', label='Baseline')\n",
    "axs[1][0].plot(bit_width_qkeras, 100*resource_usage_qkeras['RAMs'], marker='*', label='QKeras')\n",
    "axs[1][0].set_xlabel(\"Bit widths\")\n",
    "axs[1][0].set_ylabel(\"RAMs [%]\")\n",
    "axs[1][0].legend(loc='best')\n",
    "\n",
    "# Top middle: DSP usage\n",
    "axs[1][1].plot(bit_widths, 100*np.array(resource_usage['DSPs']), marker='o', label='Baseline')\n",
    "axs[1][1].plot(bit_width_qkeras, 100*resource_usage_qkeras['DSPs'], marker='*', label='QKeras')\n",
    "axs[1][1].set_xlabel(\"Bit widths\")\n",
    "axs[1][1].set_ylabel(\"DSPs [%]\")\n",
    "axs[1][1].legend(loc='best')\n",
    "\n",
    "# Top right: MLAB usage\n",
    "axs[1][2].plot(bit_widths, 100*np.array(resource_usage['MLABs']), marker='o', label='Baseline')\n",
    "axs[1][2].plot(bit_width_qkeras, 100*resource_usage_qkeras['MLABs'], marker='*', label='QKeras')\n",
    "axs[1][2].set_xlabel(\"Bit widths\")\n",
    "axs[1][2].set_ylabel(\"MLABs [%]\")\n",
    "axs[1][2].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdab1a-c3a0-421a-9265-523322f21570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "# Plot ALUT usage vs accuracy\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "axs[0].plot(100*np.array(resource_usage['ALUTs']), 100*np.array(hls_accuracy_list), marker='o', label='Baseline')\n",
    "axs[0].plot(100*resource_usage_qkeras['ALUTs'], 100*np.array(hls_accuracy), marker='*', label='QKeras')\n",
    "axs[0].set_xlabel(\"ALUTs [%]\")\n",
    "axs[0].set_ylabel(\"Accuracy [%]\")\n",
    "axs[0].set_ylim(0,100)\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "# Plot ALUT usage vs accuracy\n",
    "axs[1].plot(100*np.array(resource_usage['FFs']), 100*np.array(hls_accuracy_list), marker='o', label='Baseline')\n",
    "axs[1].plot(100*resource_usage_qkeras['FFs'], 100*np.array(hls_accuracy), marker='*', label='QKeras')\n",
    "axs[1].set_xlabel(\"FFs [%]\")\n",
    "axs[1].set_ylabel(\"Accuracy [%]\")\n",
    "axs[1].set_ylim(0,100)\n",
    "axs[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d835c-2693-4778-b19e-d0f433c7df3d",
   "metadata": {},
   "source": [
    "This is a great achievement! We have obtained a classifier with the same performance for much smaller resource usage. These \"tricks\" have a huge consequence for accelerated artificial intelligence. Not only can we vastly speed-up the inference task (e.g. for use in low latency environments like the CMS level-1 trigger), but we can substantially reduce the carbon footprint involved in the inference. This will become increasingly important in an AI-dominated world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff4d52-a7dc-4ace-9321-27a08833b1fe",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "With the quantised-aware training, investigate how low we can take the bitwidth to still achieve the same accuracy as the baseline model.\n",
    "\n",
    "Loop over several bitwidths and plot the accuracy and resource-usage as a function of bitwidth.\n",
    "\n",
    "The first block of code, plots the fixed-point vs floating-point comparison for different bitwidths to visualise the problem. \n",
    "\n",
    "You will then need to loop over different bit widths and retrain the network to see what performance can be achieved. Note, the notation for bit width is: <total number of bits, number of integer bits>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b45f2-55e6-461e-94d1-da1baa2ae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,4, figsize=(16,6))\n",
    "\n",
    "for i in range(0,8):\n",
    "    x = 0 if i < 4 else 1\n",
    "    y = i % 4\n",
    "\n",
    "    axs[x][y].plot(np.linspace(-4,4,1000), np.linspace(-4,4,1000))\n",
    "    axs[x][y].plot(np.linspace(-4,4,1000), quantized_bits(i+1,1,1,alpha=1)(np.linspace(-4,4,1000)))\n",
    "    if i > 0:\n",
    "        axs[x][y].plot(np.linspace(-4,4,1000), quantized_bits(i+1,2,1,alpha=1)(np.linspace(-4,4,1000)))\n",
    "    \n",
    "    axs[x][y].set_title(f\"Bit width: {i+1}\")\n",
    "    if i == 0:\n",
    "        axs[x][y].legend([\"Floating\", \"Quantized: <1,1>\"])\n",
    "    else:\n",
    "        axs[x][y].legend([\"Floating\", f\"Quantized: <{i+1},1>\", f\"Quantized: <{i+1},2>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e354e-64bc-4fe8-936a-c1234cbaabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: you can base your solution using the skeleton code below\n",
    "# We will keep the integer bit fixed at 1 (remaining bits are for fractional component)\n",
    "bit_widths_qkeras = [1, 2, 3, 4, 5, 6]\n",
    "N_epochs = 2\n",
    "\n",
    "hls_accuracy_qkeras_list = []\n",
    "hls_summary_qkeras_list = []\n",
    "\n",
    "for bit_width in bit_widths_qkeras:\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\" --> QKeras model with bit width: {bit_width}\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "    # Build the QKeras model, setting the first argument in the quantized_bits function to bit_width\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Initiate the optimiser, compile the QKeras model, and initiate the weights according to the baseline model\n",
    "    # -- ADD CODE HERE -- \n",
    "\n",
    "    # Train for small number of epochs (=2)\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Build the hls4ml config from the trained model, using granularity=\"name\"\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Convert and compile the model\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Calculate the accuracy and store it\n",
    "    # -- ADD CODE HERE --\n",
    "    hls_accuracy_qkeras_list.append(accuracy)\n",
    "\n",
    "    # Emulate the build and store the resource usage summary\n",
    "    # -- ADD CODE HERE --\n",
    "    with open(\"PATH-TO-REPORT/summary.ndjson\", \"r\") as f:\n",
    "        summary = ndjson.load(f)\n",
    "    hls_summary_qkeras_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79ecf7-33b6-41e4-8f04-4e20a7d9a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: extract resource usage from summary jsons\n",
    "resource_usage_qkeras_list = {}\n",
    "for summ in hls_summary_qkeras_list:\n",
    "    resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summ))[0]['columns'][1:-1]\n",
    "    available = list(filter(lambda x: x[\"name\"] == \"Available\", summ))[0]['data'][:-1]\n",
    "    estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summ))[0]['data'][:-1]\n",
    "\n",
    "    for i, resource in enumerate(resource_names):\n",
    "        r = resource.split(\" \")[0]\n",
    "        if r not in resource_usage_qkeras_list:\n",
    "            resource_usage_qkeras_list[r] = [float(estimated_resources[i])/float(available[i])]\n",
    "        else:\n",
    "            resource_usage_qkeras_list[r].append(float(estimated_resources[i])/float(available[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c18c5-2e8a-4122-9576-0eed11adeb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: plot accuracy and resource-usage vs bit-width for baseline training and for QKeras training\n",
    "fig, axs = plt.subplots(2,3, figsize=(16,8))\n",
    "\n",
    "# -- ADD CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aafeb2-0341-4602-99bf-63b1f0b69b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color:#FFCCCC\">\n",
    "    \n",
    "### Questions (assessed)\n",
    "\n",
    "* Please upload a screenshot of the graphs you have produced to show how the accuracy and resource-usage (LUTs, FFs, BRAM, DSPs, MLABs) vary as a function of the bit-widths for both the baseline and quantization-aware (QKeras) training [5 marks]\n",
    "* Write a small summary paragraph (or a few bullet points) explaining the results you have obtained [3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcec4df-060c-4650-9f70-e9f3e6e23dca",
   "metadata": {},
   "source": [
    "## Part 5: Pruning\n",
    "\n",
    "In this section we will discuss another method for reducing the resource usage, known as pruning (compression).\n",
    "\n",
    "Weight matrices in neural networks can be huge. However not every weight contributes equally to the model performance. Pruning removes the low magnitude weights with very small impact to the classification performance.\n",
    "\n",
    "On FPGAs, the weight matrices are known at compile time. Weights that are zero can be omitted by the compiler which of course reduces resource usage. \n",
    "\n",
    "To obtain sparse weight matrices, we can use L1 regularization to reduce weight magnitude. Then pruning can be scheduled to incrementaly remove and mask low magnitude weights.\n",
    "\n",
    "Let's first import the relevant functionalities from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e51db-d8f4-454c-9066-0e0d068c7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643dca9-6768-42c7-a4c5-e66b9d32735f",
   "metadata": {},
   "source": [
    "Now we will set the pruning parameters (sparsity = 75%) and prune the baseline model (without QKeras training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecbac2-19e9-4db2-ba8f-9c987d6c36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=100, frequency=100)}\n",
    "pmodel = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559677fb-3863-437c-8e1d-d97c8115eea7",
   "metadata": {},
   "source": [
    "Let's then retrain the pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110a7c7-af0b-4a97-82fe-707b0fea3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.0001)\n",
    "pmodel.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "pmodel.fit(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    "    callbacks=[pruning_callbacks.UpdatePruningStep()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36376a-4d7a-4aed-a105-701bf8e57c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip pruning wrappers before converting for inference\n",
    "pmodel = strip_pruning(pmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd31ad-d200-4d73-9f7e-d1b1de2a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print percentage sparsity (i.e. percentage of zero weights) for each layer\n",
    "for i, l in zip(pmodel.get_weights(), pmodel.layers):\n",
    "    print(f\"{np.sum(i.flatten()==0)/i.flatten().shape[0]*100:.1f}% sparsity for layer {l.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215b60c-a02e-4d8a-a46e-4595e15f8f99",
   "metadata": {},
   "source": [
    "Now we have trained a pruned neural network with 75% of weights stripped (zero), that achieves roughly the same accuracy as the baseline model. To highlight the impact of this, we will convert the model to FPGA firmware using hls4ml, and examine the resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345d5cc-55d8-4a26-b93e-730e7680c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(pmodel, granularity='model', backend='oneAPI')\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    pmodel, hls_config=config, backend='oneAPI', output_dir='model_3/hls4ml_prj', part='Agilex7'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a173d38-2d47-451a-bd81-a363ce15be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c6195-471f-49ec-bfd3-c9a73ab75bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the accuracy on the test dataset\n",
    "X_test = np.ascontiguousarray(X_test)\n",
    "y_hls = hls_model.predict(X_test)\n",
    "hls_accuracy_prune = np.sum(np.argmax(y_test, axis=1) == np.argmax(y_hls, axis=1))/y_test.shape[0]\n",
    "print(f\"Achieved test accuracy of {(100 * hls_accuracy):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8977b-97b4-426d-8c89-7bcbba8e5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emulate the build\n",
    "hls_model.build(build_type='report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9923bc6-9fc0-498a-94c3-75a971a71d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the resource usage from summary json and print\n",
    "with open(\"model_3/hls4ml_prj/build/myproject.report.prj/reports/resources/json/summary.ndjson\", \"r\") as f:\n",
    "    summary = ndjson.load(f)\n",
    "\n",
    "resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summary))[0]['columns'][1:-1]\n",
    "available = list(filter(lambda x: x[\"name\"] == \"Available\", summary))[0]['data'][:-1]\n",
    "estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summary))[0]['data'][:-1]\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~ Resource usage (pruned model) ~~~~~~~~~~~~~~\")\n",
    "for i, resource in enumerate(resource_names):\n",
    "    print(f\"--> {resource}:\")\n",
    "    print(f\"      * Available resource: {available[i]}\")\n",
    "    print(f\"      * Used resource (estimated): {estimated_resources[i]}\")\n",
    "    print(f\"      * Percentage of used resource (estimated): {100*float(estimated_resources[i])/float(available[i]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2e583-24e7-4bf5-b389-605d50d3c769",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFE5CC\">\n",
    "\n",
    "### Questions (unassessed)\n",
    "* How does the accuracy and resource usage compare to the baseline model trained in Part 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91142512-be9a-45e6-95f3-6d0f82f3f89a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Train models with different sparsity. Plot the performance and resource usage as function of model sparsity.\n",
    "\n",
    "We will investigate how large the savings in resource usage are in comparison to the baseline model before the accuracy drops.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0896c-9850-4909-8f64-cb3e6e08ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: you can base your solution using the skeleton code below\n",
    "# Loop over different sparsity values\n",
    "# Note a sparsity of zero equals the baseline model (no pruning)\n",
    "sparsities = [0., 0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "N_epochs = 5\n",
    "\n",
    "hls_accuracy_prune_list = []\n",
    "hls_summary_prune_list = []\n",
    "\n",
    "for sparsity in sparsities:\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\" --> Pruning with sparsity: {sparsity}\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "    # Apply pruning to baseline model\n",
    "    pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(sparsity, begin_step=100, frequency=100)}\n",
    "    pmodel = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # Initialise optimiser, compile and train model for 5 epochs\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Strip model\n",
    "    pmodel = strip_pruning(pmodel)\n",
    "\n",
    "    # Initialise hls4ml config and perform conversion to FPGA firmware\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Extract the accuracy on the test dataset\n",
    "    # -- ADD CODE HERE --\n",
    "    hls_accuracy_prune_list.append(accuracy)\n",
    "\n",
    "    # Emulate the build\n",
    "    # -- ADD CODE HERE --\n",
    "\n",
    "    # Extract the resource usage and append the summary to list\n",
    "    with open(\"PATH-TO-REPORT/summary.ndjson\", \"r\") as f:\n",
    "        summary = ndjson.load(f)\n",
    "    hls_summary_prune_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44efdeb-aaa0-4059-8ad4-39b820c8962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: extract resource usage from summary jsons\n",
    "resource_usage_prune_list = {}\n",
    "for summ in hls_summary_prune_list:\n",
    "    resource_names = list(filter(lambda x: x[\"name\"] == \"Estimated Resource Usage\", summ))[0]['columns'][1:-1]\n",
    "    available = list(filter(lambda x: x[\"name\"] == \"Available\", summ))[0]['data'][:-1]\n",
    "    estimated_resources = list(filter(lambda x: x[\"name\"] == \"Total\", summ))[0]['data'][:-1]\n",
    "\n",
    "    for i, resource in enumerate(resource_names):\n",
    "        r = resource.split(\" \")[0]\n",
    "        if r not in resource_usage_prune_list:\n",
    "            resource_usage_prune_list[r] = [float(estimated_resources[i])/float(available[i])]\n",
    "        else:\n",
    "            resource_usage_prune_list[r].append(float(estimated_resources[i])/float(available[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f3308-3003-4390-8d12-c0ddcbe6ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: plot accuracy and resource usage as function of sparsity\n",
    "fig, axs = plt.subplots(2,3, figsize=(16,8))\n",
    "\n",
    "# -- ADD CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f68c4f-5e5e-4bad-aa36-ea4262f29426",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color:#FFCCCC\">\n",
    "    \n",
    "### Questions (assessed)\n",
    "\n",
    "* Please upload a screenshot of the graphs you have produced to show how the accuracy and resource-usage (LUTs, FFs, BRAM, DSPs, MLABs) vary as a function of network sparsity? [5 marks]\n",
    "* Write a small summary paragraph (or a few bullet points) explaining the results you have obtained. [3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba160e0a-5698-4cc9-a91f-8a70a139aeff",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCC\">\n",
    "\n",
    "### Competition (extension)\n",
    "\n",
    "Now you have two methods for reducing the resource usage, whilst maintaining a high classification accuracy:\n",
    "\n",
    "* Quantization-aware training\n",
    "* Pruning\n",
    "\n",
    "The competition is to find the student who can **achieve the lowest resource usage for an accuracy on the test dataset of greater than 74%**.\n",
    "\n",
    "The metric for resource usage is: (Fraction of used ALUTs) + (Fraction of used FFs) i.e.\n",
    "`resource_usage['ALUTs']+resource_usage['ALUTs']`\n",
    "\n",
    "**Tip**: you should train and synthesize a model that uses both quantization-aware training and pruning. Remember that the quantization is highly customizable (different numbers for each layer), and that different methods of compression exist.\n",
    "\n",
    "Upload your best result via the same google forms. You will need to provide:\n",
    "* Written summary of your approach (bullet points are fine)\n",
    "* Print statement outputs of the global accuracy and resource-usage\n",
    "* Code used to obtain solution (everything below this block in the notebook)\n",
    "* Screenshots of any graphs you produce\n",
    "\n",
    "The results of this competition will be used to differentiate between the top performing students. There are 5 additional marks available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3863c4-d23d-4360-a100-e97d63c560f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ADD CODE HERE --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
